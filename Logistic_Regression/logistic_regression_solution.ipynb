{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will work with the Breast Cancer Wisconsin Dataset\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "The original data are taken from https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\n",
    "\n",
    "The dataset contain 30 features/variables and 569 samples. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
    "\n",
    "For each cell nucleus 10 real-valued features are computed for:\n",
    "\n",
    "a) radius (mean of distances from center to points on the perimeter)\n",
    "b) texture (standard deviation of gray-scale values)\n",
    "c) perimeter\n",
    "d) area\n",
    "e) smoothness (local variation in radius lengths)\n",
    "f) compactness (perimeter^2 / area - 1.0)\n",
    "g) concavity (severity of concave portions of the contour)\n",
    "h) concave points (number of concave portions of the contour)\n",
    "i) symmetry\n",
    "j) fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "\n",
    "For each of these feature the dataset contain 3 values:the mean, standard error and \"worst\" or largest.\n",
    "Resulting in total 30 features. For instance, feature/column 3 is Mean Radius, feature 13 is Radius Standard Error, feature 23 is Worst Radius.\n",
    "\n",
    "For every sample the target variable (y) indicates the class:\n",
    "- 0, malignant tumor\n",
    "- 1, benign tumor\n",
    "\n",
    "Class distribution: 357 benign, 212 malignant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression model as Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the excercise is to build a classifier model using logistic regression, able to distinguish between malignant and benign tumors. We will initially select only 3 features and build a model on those. Feel free to extend the model using more or others feature. Does the result improve or is it worse? Can you find a better combination of features?\n",
    "\n",
    "The 3 features we consider are mean area, mean smoothness and mean compactness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data= datasets.load_breast_cancer()\n",
    "cancer_X = cancer_data.data[:,[3,4,5]]#mean area, mean smoothness and mean compactness.\n",
    "cancer_y = cancer_data.target\n",
    "\n",
    "alldata = pd.DataFrame(data=np.c_[cancer_X, cancer_y], columns=list(cancer_data.feature_names[3:6])+['target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first visualise the data we are going to model. Is here a Logistic Model appropriate? why?\n",
    "Answer: yes, data is linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you don't have seaborn module, you need to first install it via conda or pip\n",
    "import seaborn as sns\n",
    "sns.pairplot(pd.DataFrame(alldata),hue='target',vars=list(cancer_data.feature_names[3:6]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data in training and test data. \n",
    "Feel free to change the proportion between train& test. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_test_data = int(0.2 * alldata.shape[0]) #we leave 20% of data for testing the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cancer_X_train = cancer_X[:-num_test_data]\n",
    "#cancer_X_test = cancer_X[-num_test_data:]\n",
    "\n",
    "#cancer_y_train = cancer_y[:-num_test_data]\n",
    "#cancer_y_test = cancer_y[-num_test_data:]\n",
    "\n",
    "cancer_X_train, cancer_X_test, cancer_y_train, cancer_y_test = train_test_split(\n",
    "    cancer_X, cancer_y, test_size=0.20, random_state = 99)\n",
    "\n",
    "#If you need> Pass an int to the parameter random_state for reproducible output across multiple function calls. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data is not standardised (standardise the data means transform the data so that they have mean 0 and std dev 1).\n",
    "This is a good practise, especially when data have very different ranges. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "cancer_X_train = scaler.fit_transform(cancer_X_train)\n",
    "cancer_X_test = scaler.transform(cancer_X_test)\n",
    "#did you notice that the scaler is applied both to training and test data but it is fitted ONLY on training data?\n",
    "#can you guess why?\n",
    "#answer: if you would have computed the scaler function using also test data, part of this information would have been implicitly\n",
    "#been included in the training data, because they would have been scaled with coefficients who \"knew\" the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! All data have been prepared! Now it is your moment.\n",
    "\n",
    "Complete the following steps of modelling by adding the necessary code under where #ADD YOUR CODE ## is indicated. \n",
    "\n",
    "First create the logistic regression model using the sklearn function:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()#### ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train (=fit) the model on the training data (cancer_X_train, cancer_y_train).\n",
    "\n",
    "Use here the methods listed for the LogisticRegression classifier, you find them in the sklearn link above or in the lecture slide 15. Remember that the methods (python function) need to be called from the correspondent object. \n",
    "So for example if LogisticRegression had a method called \"compute_something(a,b)\", I would need to call it as follows:\n",
    "log_reg.compute_something(mya, myb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(cancer_X_train, cancer_y_train)#### ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print now the model parameters (coefficients (=theta1, theta2....) and intercept (=theta_0)\n",
    "As usual find the correct method as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_reg.coef_)\n",
    "print(log_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to predict the class outcome for the test data (cancer_X_test). \n",
    "If you would want to predict the probability you would use log_reg.predict_proba, which method do you use to predict the discrete class to which the test samples belong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = log_reg.predict_proba(cancer_X_test)\n",
    "\n",
    "y_pred = log_reg.predict(cancer_X_test) #ADD YOU CODE HERE####\n",
    "\n",
    "pd.DataFrame(np.c_[y_proba,y_pred], columns = ['Probability '+list(cancer_data.target_names)[0], \n",
    "                                               'Probability '+list(cancer_data.target_names)[1],\n",
    "                                                'Predicted Class'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the accuracy of our model using the TEST data, \n",
    "both input (cancer_X_test) and known exact true class(cancer_y_test). \n",
    "\n",
    "As usual, check among the methods of LogisticRegression the right one to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = log_reg.score(cancer_X_test, cancer_y_test) #ADD YOU CODE HERE####\n",
    "print(\"The accuracy of your model is {0}%\".format(np.round(mean_accuracy*100),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Try now to select new or more feature from the original dataset and see if there are features which can improve the model or that make it worse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Classification problem it is always good to not just consider the accuracy of your model but to visualise also the confusion matrix which show you how many cases you correclty classified but also if your mislabeled cases were false positive or false negative.\n",
    "We will discuss about this more next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay# for this plot sklearn >= 1.0.1 is needed\n",
    "\n",
    "cm = confusion_matrix(cancer_y_test, y_pred, labels=log_reg.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=log_reg.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: Notebook adapted from A.Geron, Hands On ML with Scikit-Learn, Keras und Tensorflow, O'Reilly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSF_HS22",
   "language": "python",
   "name": "dsf_hs22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
